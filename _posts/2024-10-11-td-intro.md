---
title: "Terrain Diffusion: Introduction"
excerpt_separator: "<!--more-->"
categories:
  - terrain-diffusion
tags:
  - generative models
  - diffusion
  - ai
---

Procedural generation has been a staple of computer programming for decades. In fact, cellular automata were some of the first programs I ever wrote.
The ability for computers to create fascinating structures with minimal effort is an incredible power, and it can be used to create fascinating
programs. Most commonly, it's used in video games to create vast worlds with minimal effort. Unfortunately, game developers usually have to choose between
automated terrain generation, which allows for infinite variation and gameplay, or hand-crafted terrain, which is far more interesting, but also massively
limits variation. Putting in some effort, procedural generation can become significantly more sophisticated, but it is still limited to algorithms that
can be expressed in code and run in real time.

Perlin noise is a popular method for creating natural looking terrain, but looks far from realistic. Adding erosion and other effects help significantly,
but variation is still limited, and more importantly, results can take far too long to be practical. The development of generative models presents a 
unique opportunity to bridge this gap. By training models to generate realistic terrain, we can create landscapes that can approach or possibly exceed
the beauty of hand-crafted terrain, while still allowing for the variation and vastness of procedurally generated terrain.

This past few weeks, I've been working on a project that aims to do just that. In this post, I'll detail the basic method I've been exploring,
and in future posts, I'll detail some of the more advanced techniques I've been working on to turn this basic method into a powerful terrain generation
tool.

![some generated terrain]({{ site.url }}{{ site.baseurl }}/assets/images/example_terrain.png)

## The Setup

Our training data will consist of 2D slices of elevation data from the real world.
For now, I'll skip the process of creating the dataset, and just say that we have 1024x1024 crops of elevation data.
I will post details about how I created the dataset at the end of this post, since it is not particularly interesting. 
To begin, we will focus on the task of just generating a single image of terrain elevation.

Although you can begin training models at any scale, it is a good idea to begin with as large a scale as possible.
This allows the model to learn about the larger features of terrain, which is important for generating realistic results.
If we were to focus on, say, generating terrain in one kilometer by one kilometer patches, the model would likely learn to generate
pretty good one kilometer by one kilometer terrain, but would have no idea how to extrapolate that knowledge to generate entire continents.
Instead, we will start by focusing on generating very large scale terrain, like that of small countries, and then scale up from there.

Here is an example of some of the 64x64 (the images are resized from 1024x1024) terrain slices that we will use for the first stage of training.
This isn't too different from the actual data distribution, but I removed some of the more boring slices (like flat plains).
At this scale, every pixel is roughly 4 kilometers by 4 kilometers. One of the images (i think) shows the terrain of Gibraltar.

The data also only shows terrain with more than 10% of the area above sea level.
We will initially prioritize training on land since it's harder to generate.
Later, the model can be fine-tuned to generate underwater terrain more often.
You may notice that gray and white are more common in the training data than in the example above.
I will discuss this in more detail in a later post.

![Terrain grid]({{ site.url }}{{ site.baseurl }}/assets/images/real_terrain/terrain_grid.png)
*Examples of terrain data. The color gradient is the same as in the image above: Roughly scaled so that yellow is near sea level, green is in the hundreds of meters,
brown is in the thousands of meters, gray in the several thousands of meters, and white is reserved for only the highest peaks.*

The next step is training the model, but first there are a few things to consider that make training a bit more complicated than usual.

First, terrain has a lot of variance. The deepest underwater trenches can go as far as 11km below sea level. Meanwhile, mountains can go up to over 8km above sea level.
While we can normalize the data to ensure this variance is standard, this has the side effect of massively amplifying small errors when we later unnormalize the data. I will explain how to get around this later.

Second, we eventually want to begin tiling our terrain. In order to ensure this works correctly, we need to train the model to effectively generate crops of a larger image.
This way, we can later generate many crops together to form a larger image. Thankfully, our training data is already in the form of crops of the earth, so we don't
have to worry about this too much. Still, we need to be careful that any modifications we make to the data are tileable.

With these considerations in mind, we can now move on to training the model.

## The Model

I initially started with a simple U-Net architecture from HuggingFace. While the model worked decently,
it was clear that it's not quite state of the art. I spent a while searching for better architectures, and eventually stumbled upon
[EDM2](https://arxiv.org/abs/2312.02696), EDM2's architecture is not too different from the basic U-Net architecture, 
but it implements a few interesting techniques that I find relatively rare in most modern models I see today. 
EDM2 completely removes all group norm layers from the architecture, and instead explicity normalizes weights to ensure a norm of 1.
This is the first time I've actually seen weight normalization being used in a modern model, as batch norm, layer norm, and group norm seem 
to be the most common normalization layers used today. Nonetheless, EDM2 uses this technique to great success, and achieves massive
improvements in FID with the technique. With this, EDM2 achieves nearly state of the art results with a fraction of the compute
in a fashion reminiscent of EfficientNet, just for diffusion models.

EDM2 should serve as a great starting point for our model, and will hopefully lead to real-time performance. It's also convenient
for me, since I will be running all my experiments on a single RTX 3090 Ti.

## Laplacian Pyramids

Like I mentioned earlier, terrain has a lot of variance. Instead of just normalizing all the data at once, I decided to separate the
data into various different scales, and normalize each scale individually. This should allow the model learn to generate high frequency 
features like rivers or small canyons separately from low frequency features like continents and mountain ranges.

To solve this, I use a technique called Laplacian pyramids. The idea is to progressively downsample and (optionally) blur the data.
The downsampled images store the lower frequency components of the image. Then, the high frequency details can be constructed
by taking the difference between the original image and the downsampled image. The downsampled image is returned back to the original size
with some form of interpolation. I believe bicubic interpolation is the ideal choice here, as it ensures the image is as smooth as possible.

![example of a laplacian pyramid]({{ site.url }}{{ site.baseurl }}/assets/images/laplacian_pyramid.png)
*A two-level Laplacian Pyramid. Left: The original image. Center: The high frequency details. Right: The low frequency components.*

Although there is room to experiment with different pyramid sizes and levels, I have so far found that the best results come from
a two-level pyramid, where second level is downsampled by a factor of 8 and has no gaussian blur applied. With this setup,
the high frequency component has its standard deviation reduced to a mere 155 meters, while retaining most of the detail.
This means the model can focus on learning the complex high frequency features, while the low frequency features are established
early in the diffusion process.

I opted for no blur, for one just to save on compute, but also because it lowers the dependence on data outside of the crop*, which
could potentially cause issues when tiling in the future. I also found that we could denoise the data much more efficiently without
a blur, which I'll discuss in more detail in a later post. In order to simulate larger crops when downsampling, I padded 
the images with the edge values, and found that this worked well enough.

## Training

My first training sessions were on the original EDM2 repository. I began with the default training settings, except significantly lowered
the batch size from 2048 to just 96, although I did use 2 gradient accumulation steps. Despite the massively reduced batch size,
training was pretty stable, and I was able to get pretty good results within about a week of training.

When training at this scale, there are only about ~1500 images with more than 10% land in the entire dataset. Overfitting was a significant issue.
To overcome this, we can rotate and flip the images in any way, which effectively multiplied the dataset size by a factor of 8 to 12000.
Even so, overfitting can be a problem. To further reduce overfitting, I took 64x64 crops out of higher resolution images (resized to 256x256 and 1024x1024), 
and applied the same laplacian pyramid encoding to them, although with different normalization to account for the lower levels of variance in "zoomed in" images.

Since each 256x256 image has 16 non-overlapping 64x64 crops, and the 1024x1024 images have 256, this simulates a massive boost in dataset size.
To ensure that this extra data does not leak into the inference stage, we can add a label that indicates to the model what resolution the original image is.
During inference, we just pass in the label used for 64x64 images. Following this method, overfitting is essentially completely eliminated. 


![fake terrain]({{ site.url }}{{ site.baseurl }}/assets/images/fake_terrain/fake_grid.png)
*Fake terrain generated by the model. All samples are 64x64 and normalize to utilize the entire color range.*

I later began rewriting the EDM2 repository to be easier to work with, and specialize it for terrain generation.
After modifying some of the training parameters and adding denoising functionality, I ended up with samples list this:

![current terrain]({{ site.url }}{{ site.baseurl }}/assets/images/fake_terrain/current_grid.png)
*Fake terrain generated by the current model. All samples are 64x64 and normalize to utilize the entire color range.*

The model is still far from perfect, but I am confident it will continue to improve as I experiment with different techniques.



## Appendix

### Creating the Dataset

I initialy looked for various ways to get earth data for this project, but eventually settled on just using EarthEngine. EarthEngine limits free users
to 2 exports at a time, so it can take a while to get a large number of exports, but this is still just a fraction of the time it takes to train on the
data anyways. For the training data I show here, I exported 1024x1024 images at a resolution of 240 meters per pixel.

The images are separated by a latitude of 3 degrees, while the longitude varies by the equation `3 * 0.75 / cos(lat)`.
Multiplying by 0.75 was just the number I found minimized overlap, while dividing by cosine accounts for the fact that the lines of longitude converge at the poles.
The dataset I used was [SRTM](https://developers.google.com/earth-engine/datasets/catalog/CGIAR_SRTM90_V4) data, but I also replaced areas below sea level with
[ETOPO1](https://developers.google.com/earth-engine/datasets/catalog/NOAA_NGDC_ETOPO1), which has low resolution bathymetry data.

### Some early challenges

I initially tried training with an Imagen-style pipeline. However, there were a few issues I ran into early on.
I couldn't use the default noise schedule. Cosine schedule in particular does not seem to work well for
non-image data. A linear schedule worked much better. Even then, the default min/max noise levels were not
well suited for training in a latent space, and required much more tuning to get correct.
I also found that while most people just choose the default uniform distribution for noise when training,
this is usually a pretty bad choice. Training speed, stability, and quality can all be improved massively by 
prioritizing the noise levels that are most relevant, which are usually in the middle of the range. 
Large noise levels are easy to predict and require minimal training, 
while small noise levels are difficult to learn and provide minimal improvements to FID. In fact,
min-SNR finds huge improvements in training time and FID by lowering the loss weight of small noise levels.
They found some improvements with EDM, which prioritizes the middle noise levels, but these improvements
were not nearly as significant as with most other models.
